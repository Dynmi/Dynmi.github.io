---
layout: post
comments: true
title:  "Diagonal Matrix and Orthogonal Matrix"
excerpt: "..."
date:   2022-01-06 00:37:00
mathjax: true
---


## 以线性变换视角看矩阵

用 $M_{mn}$ 表示一个m行n列的矩阵。
建议您默认以列向量的视角来看一个矩阵。用 $m_1$，$m_2$，…，$m_n$ 表示 $M_{mn}$的列向量。因此也可以用 [$m_1$，$m_2$，…，$m_n$] 表示 $M_{mn}$。
这里先选取简单情况：M为nn的方阵。对于任意nx1的向量 $a$，$b$ = $Ma$ 可以视为对$a$做了一个线性变换：将$a$的n个单位基向量分别拉伸到 $m_1$，$m_2$，…，$m_n$，得到向量 $b$。


## 最直白的矩阵——对角矩阵、正交矩阵

对角矩阵与正交矩阵都是方阵，因为运算便捷、可解释性强，很受人喜欢。
就像自然数是研究数论的基石一样，对角矩阵和正交矩阵就是研究线性代数的基石。

对角矩阵就很好理解了，即只对每个基向量进行压缩拉伸，而不扭转旋转。

正交矩阵，即对整组正交基保持原状作旋转，而不压缩拉伸。

正交矩阵有几个性质，它们将加速你遇到正交矩阵后的问题处理速度：
- $A^T = A^{-1}$
- $AA^T = A^TA = I$
- A的n个列向量都是单位向量且彼此正交
- A的n个行向量都是单位向量且彼此正交
- $|det(A)| = 1$（当然也就表示rank(A)为n）

这些都很好理解，而且证明过程很简单，这里就不写出来了。


## 对称矩阵的特征值分解

任意满秩方阵都可以进行特征值分解，这很简单就不在这里证明了。
实际应用中，出场率比较高的是对称矩阵。

对于对称矩阵M，必有正交矩阵 $A$，使得 $M=A_{nn}diag_{n}(\lambda)A_{nn}^{T}$。

即，任意对称矩阵的线性变换可以看作依次进行旋转变换 $A^{T}$、伸缩变换 $diag_{n}(\lambda)$、逆旋转变换 $A$。

这是对以上的证明：
> + **充要条件**：$M$为n行n列对称矩阵，|det(M)| > 0。
> + **得到结论**：存在正交矩阵 $A$和对角矩阵 $diag(\lambda)$，使得 $M=Adiag_{n}(\lambda)A^{-1}$
> + **证明过程**：
>> + 如果满秩矩阵M是一个对称矩阵，那就是说$M=M^{T}$。
>> + 已知M可以分解为$M=A_{nn}diag_{n}(\lambda)A_{nn}^{T}$，那么$MA_{nn}^{T}=A_{nn}diag_{n}(\lambda)$
>> + 进而有 $A^{T}M = A^{T}M^{T} = (MA)^{T} = (Adiag(\lambda))^{T} = diag(\lambda)A^{T}$，
>> + 进而有 $M = (A^{T})^{-1}diag(\lambda)A^{T}$
>> + 结合 $M=A_{nn}diag_{n}(\lambda)A_{nn}^{-1}$，即有$A^{T} = A^{-1}$。
>> + 故A为正交矩阵，故 $M=A_{nn}diag_{n}(\lambda)A_{nn}^{T}$

EVD算法：

---

**Input**: M

**Output**: A, $diag_{n}(\lambda)$

计算$\lambda$，根据$det(M-\lambda I) = 0$


将$\lambda$的不同解分别代入$(M-\lambda I)a = 0$，计算出各自对应的向量$a$。组成矩阵A。

---

Ok，相信你已经能接受认可以上所述。我们从线性代数的角度找到了这个规律，那就可以在任何满足充要条件的场景下使用这个规律了。


[PageRank算法](https://zh.wikipedia.org/wiki/PageRank)


## 任意矩阵的奇异值分解

如果你学会了特征值分解，但没有听过奇异值分解，看到缺乏美感的非方阵，你一定会有这种灵感直觉：非方阵即mn的矩阵，应该也能分解成对角矩阵和正交矩阵吧？ 
没错，接下来我们一起研究并证实确认你的直觉是对的。

任意线性变换都可以分解为：依次进行一个旋转变换、一个伸缩变换、一个旋转变换。
即，对于任意矩阵 $M_{mn}$，都存在正交矩阵 $U_{mm}$和$V_{nn}$，伪对角矩阵 $\Sigma_{mn}$，使得 $M=U \Sigma V^{T}$。

这里先给出这条规律的完整陈述和证明：
> + **充要条件**：任意mn的矩阵M
> + **得到结论**：存在正交矩阵 $U_{mm}$、正交矩阵 $V_{nn}$、伪对角矩阵 $\Sigma_{mn}$，$M=UBV^{T}$
> + **证明过程**：
>> + 易证 $(AB)^{T} = B^{T} A^{T}$，故有$(M^{T}M)^{T} = M^{T}M$。从这里看出 $M^{T}M$是对称矩阵；
>> + 因此$M^{T}M$可以进行正交分解，即存在正交阵 $V_{nn}$和对角阵 $diag_{n}(\lambda)$使得 $M^{T}M = V_{nn} diag_{n}(\lambda) V_{nn}^{T}$；
>> + 把上式进一步推导，存在正交阵 $U_{mm}$和伪对角矩阵 $\Sigma_{mn}$（满足$\Sigma \Sigma^{T} = diag_{n}(\lambda)$），使得 $M^{T}M = V_{nn} (\Sigma_{mn})^{T} U_{mm}^{T} U_{mm} \Sigma_{mn} V_{nn}^{T} = (U_{mm} \Sigma_{mn} V_{nn}^{T})^{T} (U_{mm} \Sigma_{mn} V_{nn}^{T})$；
>> + 从而有 $M=U \Sigma V^{T}$

SVD算法：

---

**Input**: M

**Output**: U, $\Sigma$, V

计算$S$ = MULTIPLY( $M$, $M^{T}$ )

计算$V$, $diag_{n}(\lambda)$ = EVD( $S$ )

计算$\Sigma_{mn}$，根据$(\Sigma_{mn})^{T} \Sigma_{mn} = diag_{n}(\lambda)$

计算$U = MV(\Sigma)^{T}$

---

Ok，相信你已经能接受认可以上所述。

利用 $M_{mn} = \sigma_{1} u_{1}^{T}v_{1} + \sigma_{2} u_{2}^{T}v_{2} + ... + \sigma_{n} u_{n}^{T}v_{n}$，就可以将 $M_{mn}$视为多项之和。因为 $u$和 $v$都是单位向量，那么 $\sigma$大小就代表了这个项的权重占比。

r(M)近似公式， $Argmin_{r(X) = r(M)}|M-X| = U^{-}_{nn}\Sigma_{nn}V_{nn}^{T}$，其中$U^{-}_{nn}$是$U_{mn}$在长边m上的n截断。

我们从线性代数的角度找到了这个规律，那就可以在任何满足充要条件的场景下使用这个规律了。

[主成分分析](https://zh.wikipedia.org/zh-hans/%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90)

[图像降噪](https://ieeexplore.ieee.org/abstract/document/7067415/)